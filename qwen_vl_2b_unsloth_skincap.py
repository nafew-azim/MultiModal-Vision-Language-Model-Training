# -*- coding: utf-8 -*-
"""qwen_vl_2b_unsloth_skincap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4yTILFhMjcpdgsJLdBYgibsCtskAZHv
"""

##############################
# INSTALL LIBRARIES
##############################
import os
os.system("pip install git+https://github.com/huggingface/transformers.git@main")
os.system("pip install -q datasets")
os.system("pip install rouge-score")
os.system("pip install --upgrade datasets")
os.system("pip install huggingface_hub datasets")
os.system("pip install 'kagglehub[pandas-datasets]'")
os.system("pip install bitsandbytes")
os.system("pip install unsloth")
os.system("pip install sentencepiece protobuf 'datasets>=3.4.1' huggingface_hub hf_transfer")

##############################
# IMPORTS
##############################
import math
import torch
import pandas as pd
from datasets import load_dataset
from unsloth import FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from transformers import TextStreamer
from trl import SFTTrainer, SFTConfig

os.environ["TORCH_DYNAMO_DISABLE"] = "1"

##############################
# MODEL SETUP
##############################
def load_model():
    model, tokenizer = FastVisionModel.from_pretrained(
        "unsloth/Qwen2-VL-2B-Instruct-bnb-4bit",
        load_in_4bit=True,
        use_gradient_checkpointing="unsloth"
    )
    model = FastVisionModel.get_peft_model(
        model,
        finetune_vision_layers=True,
        finetune_language_layers=True,
        finetune_attention_modules=True,
        finetune_mlp_modules=True,
        r=16,
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        random_state=3407,
        use_rslora=False,
        loftq_config=None,
    )
    return model, tokenizer

##############################
# LOAD AND PROCESS DATA
##############################
def prepare_data():
    dataset = load_dataset("joshuachou/SkinCAP", split="train").select(range(4000))
    captions_df = pd.read_excel("/kaggle/input/skincap-captions/skincap_v240715.xlsx", header=1)
    dataset = dataset.map(lambda example, idx: {'caption': captions_df.iloc[idx]['caption_en']}, with_indices=True)

    train_test_split = dataset.train_test_split(test_size=0.2)
    train_dataset = train_test_split['train']
    temp_dataset = train_test_split['test']
    val_test_split = temp_dataset.train_test_split(test_size=0.5)
    val_dataset = val_test_split['train']
    test_dataset = val_test_split['test']

    def convert_to_conversation(sample):
        return {
            "messages": [
                {"role": "user", "content": [
                    {"type": "text", "text": "Provide a description for this image.."},
                    {"type": "image", "image": sample["image"]}
                ]},
                {"role": "assistant", "content": [
                    {"type": "text", "text": sample["caption"]}
                ]}
            ]
        }

    converted_train_dataset = [convert_to_conversation(sample) for sample in train_dataset]
    return converted_train_dataset, val_dataset, test_dataset

##############################
# TRAINING
##############################
def train_model(model, tokenizer, train_dataset):
    FastVisionModel.for_training(model)
    steps_per_epoch = math.ceil(len(train_dataset) / 4)
    save_steps = 5 * steps_per_epoch

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        data_collator=UnslothVisionDataCollator(model, tokenizer),
        train_dataset=train_dataset,
        args=SFTConfig(
            per_device_train_batch_size=4,
            gradient_accumulation_steps=1,
            warmup_steps=5,
            num_train_epochs=20,
            learning_rate=5e-5,
            logging_steps=1,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="/kaggle/working/qwen2_vl_2b_checkpoints",
            report_to="none",
            remove_unused_columns=False,
            dataset_text_field="",
            dataset_kwargs={"skip_prepare_dataset": True},
            max_seq_length=2048,
            save_strategy="steps",
            save_steps=save_steps,
            push_to_hub=True,
            hub_strategy="every_save",
        ),
    )
    return trainer

##############################
# SAVE MODEL
##############################
def save_model(model, tokenizer):
    path = "/kaggle/working/finetuned_qwen_vl_2b_20_epoch_skincap"
    model.save_pretrained(path)
    tokenizer.save_pretrained(path)
    model.push_to_hub("finetuned_qwen_vl_2b_20_epoch_skincap")
    tokenizer.push_to_hub("finetuned_qwen_vl_2b_20_epoch_skincap")
    print(f"Model and tokenizer saved to {path} and pushed to hub.")

##############################
# MEMORY STATS
##############################
def print_gpu_memory():
    gpu_stats = torch.cuda.get_device_properties(0)
    max_reserved = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
    print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
    print(f"{max_reserved} GB of memory reserved.")

##############################
# GENERATE SAMPLE
##############################
def generate_sample_predictions(model, tokenizer, val_dataset):
    FastVisionModel.for_inference(model)
    image = val_dataset[0]["image"]
    instruction = "Provide a description for this image.."
    messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": instruction}]}]
    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
    inputs = tokenizer(image, input_text, add_special_tokens=False, return_tensors="pt").to("cuda")
    text_streamer = TextStreamer(tokenizer, skip_prompt=True)
    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1)

##############################
# MAIN EXECUTION
##############################
def main():
    model, tokenizer = load_model()
    train_dataset, val_dataset, _ = prepare_data()
    trainer = train_model(model, tokenizer, train_dataset)
    print_gpu_memory()
    save_model(model, tokenizer)
    generate_sample_predictions(model, tokenizer, val_dataset)

if __name__ == "__main__":
    main()