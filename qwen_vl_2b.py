# -*- coding: utf-8 -*-
"""qwen_vl_2b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4yTILFhMjcpdgsJLdBYgibsCtskAZHv
"""

import torch
print(torch.__version__)
print(torch.version.cuda)
print(torch.cuda.is_available())

torch.cuda.empty_cache()
torch.cuda.ipc_collect()
print("Cleared torch CUDA memory.")

!nvidia-smi

!gpustat

!pip show torch torchvision torchaudio unsloth trl unsloth-zoo sentencepiece transformers xformers datasets peft trl huggingface_hub kagglehub qwen-vl-utils | grep -E "Name|Version"

torch._dynamo.config.suppress_errors = True
torch._dynamo.config.disable = True

!huggingface-cli login --token TOKEN_HERE

!pip install git+https://github.com/huggingface/transformers
!pip install datasets
!pip install huggingface_hub
!pip install kagglehub
!pip install qwen-vl-utils
!pip install peft
!pip install trl

import os
import torch
from datasets import load_dataset
from transformers import (
    Qwen2VLForConditionalGeneration,
    AutoTokenizer,
    AutoProcessor,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    GenerationConfig,
    TextStreamer
)
from qwen_vl_utils import process_vision_info
from peft import LoraConfig, get_peft_model
import gc

from datasets import load_dataset

dataset = load_dataset("eltorio/ROCOv2-radiology")

train_ds = dataset["train"]
val_ds = dataset["validation"]
test_ds = dataset["test"]

print(f"Train set size: {len(train_ds)}")
print(f"Validation set size: {len(val_ds)}")
print(f"Test set size: {len(test_ds)}")

#raw examples
print("Example image URL from train dataset at given index:", train_ds[2]["image"])
print("Example caption from train dataset at given index:", train_ds[2]["caption"])

instruction = "Provide a description for this image."

def to_conv(sample, instruction):
    return {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": sample["image"]},
                    {"type": "text", "text": instruction}
                ]
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": sample["caption"]}]
            }
        ]
    }



train_conv = train_ds.map(
    to_conv,
    fn_kwargs={"instruction": instruction},
    remove_columns=train_ds.column_names,
    num_proc=8,
    desc="Mapping training set"
)

val_conv = val_ds.map(
    to_conv,
    fn_kwargs={"instruction": instruction},
    remove_columns=val_ds.column_names,
    num_proc=8,
    desc="Mapping validation set"
)

print("\nConverted train dataset example at index 0:")
print(train_conv[0])

print("\nCheck content inside messages:")
for message in train_conv[0]["messages"]:
    print(message)

import os

# Directory to store image files
image_save_dir = "./roco_images"
os.makedirs(image_save_dir, exist_ok=True)

# Save images to disk and replace 'image' with the file path
def save_images_and_get_paths(example, idx):
    image_path = os.path.join(image_save_dir, f"train_img_{idx}.png")
    example["image"].save(image_path)
    return {"image_path": image_path, "caption": example["caption"]}

# Save for train, val, and test
train_ds_with_paths = train_ds.map(save_images_and_get_paths, with_indices=True, desc="Saving train images")
val_ds_with_paths = val_ds.map(save_images_and_get_paths, with_indices=True, desc="Saving val images")

def to_conv(sample, instruction):
    return {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": sample["image_path"]},  # âœ… Use path, not bytes
                    {"type": "text", "text": instruction}
                ]
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": sample["caption"]}]
            }
        ]
    }

train_conv = train_ds_with_paths.map(
    to_conv,
    fn_kwargs={"instruction": instruction},
    remove_columns=train_ds_with_paths.column_names,
    num_proc=8,
    desc="Mapping training set with paths"
)

val_conv = val_ds_with_paths.map(
    to_conv,
    fn_kwargs={"instruction": instruction},
    remove_columns=val_ds_with_paths.column_names,
    num_proc=8,
    desc="Mapping validation set with paths"
)

print("\nConverted train dataset example at index 0:")
print(train_conv[0])

print("\nCheck content inside messages:")
for message in train_conv[0]["messages"]:
    print(message)

model_name = "Qwen/Qwen2-VL-2B-Instruct"

processor = AutoProcessor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)



chat_template_qwen2vl = {
    "chat_template": """{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}
{% for message in messages %}
{% if loop.first and message['role'] != 'system' %}<|im_start|>system
You are a helpful assistant.<|im_end|>
{% endif %}<|im_start|>{{ message['role'] }}
{% if message['content'] is string %}{{ message['content'] }}<|im_end|>
{% else %}{% for content in message['content'] %}
{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}
{% set image_count.value = image_count.value + 1 %}
{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>
{% elif content['type'] == 'video' or 'video' in content %}
{% set video_count.value = video_count.value + 1 %}
{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>
{% elif 'text' in content %}{{ content['text'] }}{% endif %}
{% endfor %}<|im_end|>
{% endif %}
{% endfor %}
{% if add_generation_prompt %}<|im_start|>assistant
{% endif %}"""
}

tokenizer.chat_template = chat_template_qwen2vl["chat_template"]


model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    lora_dropout=0.0,
    bias="none",
    task_type="SEQ_2_SEQ_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "mlp"]
)
model = get_peft_model(model, lora_config)

from torch.utils.data import Dataset

class QwenVLChatDataset(Dataset):
    def __init__(self, dataset, tokenizer, processor):
        self.dataset = dataset
        self.tokenizer = tokenizer
        self.processor = processor

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        messages = item["messages"]
        prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[prompt],
            images=image_inputs,
            videos=video_inputs,
            return_tensors="pt",
            padding=True
        )
        target_text = messages[-1]["content"][0]["text"]
        labels = self.tokenizer(
            target_text, return_tensors="pt", padding=True
        ).input_ids
        inputs["labels"] = labels
        return {k: v.squeeze(0) for k, v in inputs.items()}

train_dataset = QwenVLChatDataset(train_conv, tokenizer, processor)
val_dataset = QwenVLChatDataset(val_conv, tokenizer, processor)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True,
    return_tensors="pt"
)

training_args = TrainingArguments(
    output_dir="./qwen2vl_roco_lora",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    num_train_epochs=10,
    learning_rate=2e-5,
    logging_steps=1,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=3407,
    bf16=True,
    save_strategy="epoch",
    save_total_limit=2,
    remove_unused_columns=False,
    push_to_hub=True,
    hub_model_id="finetuned-qwen2vl-2b-roco",
    report_to="none"
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator
)

trainer.train()

model.eval()

image = val_ds[0]["image"]
messages = [
    {"role": "user", "content": [
        {"type": "image", "image": image},
        {"type": "text", "text": "You are an expert radiographer. Describe accurately what you see."}
    ]}
]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
img_in, vid_in = process_vision_info(messages)
inputs = processor(
    text=[prompt],
    images=img_in,
    videos=vid_in,
    return_tensors="pt",
    padding=True
).to("cuda")

streamer = TextStreamer(tokenizer, skip_prompt=True)
gen_cfg = GenerationConfig.from_pretrained(model_name)
gen_cfg.cache_implementation = "dynamic"

generated = model.generate(
    **inputs,
    generation_config=gen_cfg,
    streamer=streamer,
    max_new_tokens=128,
    use_cache=True,
    temperature=1.5,
    min_p=0.1
)

decoded = processor.batch_decode(
    generated[:, inputs.input_ids.shape[1]:],
    skip_special_tokens=True,
    clean_up_tokenization_spaces=False
)
print(decoded[0])

model_id = "finetuned_qwen_vl_2b_10_epoch_roco_v2_lora_model"
save_path = f"/kaggle/working/{model_id}"

model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print(f"Model and tokenizer saved to {save_path}")

# Push to Hub manually
model.push_to_hub(model_id)
tokenizer.push_to_hub(model_id)
print(f"Model and tokenizer pushed to Hub {model_id}")

# Only if you're using PEFT (LoRA)
model.save_pretrained_merged(f"{save_path}_merged", tokenizer)
model.push_to_hub_merged(f"{model_id}_merged", tokenizer)